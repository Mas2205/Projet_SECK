{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECK Mouhamadou Abdoulaye\n",
    "# abdouseck.tiv@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugwg3pPhbxr-"
   },
   "source": [
    "## Deep Learning et réseau RNN (Recurent Neural Network)\n",
    "\n",
    "L'objectif de cette manipulation est de vous montrer le potentiel du deep learning en NLP et plus particulirerment en NLG (Natural Language Generation)\n",
    "\n",
    "\n",
    "#### Définition\n",
    "Les réseaux de neurones récurrents, sont une classe de réseaux de neurones qui permettent aux prédictions antérieures d'être utilisées comme entrées, par le biais d'états cachés (en anglais hidden states)\n",
    "Un RNN a la particularité d'avoir une memoire sur les bonnes predictions de la sequence de sortie (en fonction de la sequence d'entrée).\n",
    "Les modèles RNN sont surtout utilisés dans les domaines du traitement automatique du langage naturel et de la reconnaissance vocale.\n",
    "\n",
    "\n",
    "![Title](https://www.i2tutorials.com/wp-content/media/2019/09/Neural-network-62-i2tutorials.png)\n",
    "\n",
    "\n",
    "![Title](https://miro.medium.com/max/600/1*pQ2tm6Mirdrf6hqwfYXb0g.gif)\n",
    "\n",
    "#### Variantes aux RNNs traditionnels,\n",
    "Les unités de porte récurrente (en anglais Gated Recurrent Unit) (GRU) et les unités de mémoire à long/court terme (en anglais Long Short-Term Memory units) (LSTM) où le LSTM peut être vu comme étant une généralisation du GRU. Ils apaisent le problème du \"gradient qui disparait\", rencontré par les RNNs traditionnels\n",
    "\n",
    "\n",
    "\n",
    "#### Sequence to Sequence \n",
    "\n",
    "La séquence à séquence (Seq2Seq) consiste à entraîner des modèles pour convertir des séquences d'un domaine (par exemple des phrases en anglais) en séquences d'un autre domaine (par exemple les mêmes phrases traduites en français).\n",
    "\n",
    "Cela peut être utilisé pour la traduction automatique ou pour la réponse aux questions sans réponse (générant une réponse en langage naturel à partir d'une question en langage naturel) - en général, il est applicable chaque fois que vous avez besoin de générer du texte.\n",
    "\n",
    "Il existe plusieurs façons de gérer cette tâche, dont celle utilisant des RNN.\n",
    "\n",
    "\n",
    "\n",
    "Dans le cas général, les séquences d'entrée et les séquences de sortie ont des longueurs différentes (par exemple, traduction automatique) et la séquence d'entrée entière est nécessaire pour commencer à prédire la cible. Cela nécessite une configuration plus avancée, ce à quoi les gens se réfèrent généralement lorsqu'ils mentionnent des «modèles de séquence à séquence» sans autre contexte. Voilà comment cela fonctionne:\n",
    "\n",
    "+ Une couche RNN (ou plusieurs couches) joue le rôle d'\"encodeur\": elle traite la séquence d'entrée et renvoie son propre état interne. Notez que nous rejetons les sorties de l'encodeur RNN, ne **récupérant que l'état**. Cet état servira de \"contexte\", ou \"conditionnement\", du décodeur à l'étape suivante.\n",
    "\n",
    "+ Une autre couche RNN (ou plusieurs couches) fait office de \"décodeur\": elle est entraînée pour prédire les caractères suivants de la séquence cible, étant donné les caractères précédents de la séquence cible. Plus précisément, il est formé pour transformer les séquences cibles en décalées d'un pas de temps dans le futur. Il est important de noter que l'encodeur utilise comme état initial les vecteurs d'état de l'encodeur, ce qui permet au décodeur d'obtenir des informations sur ce qu'il est censé générer. En effet, le décodeur apprend à générer des cibles [t + 1 ...] en fonction de [... t], conditionnées à la séquence d'entrée (hidden state).\n",
    "\n",
    "Nous allons aborder les model de type sequence to sequence et en particulier le Long Short Term Memory (LSTM) tres utilisé egalement en Time series.\n",
    "\n",
    "![ee](https://metalblog.ctif.com/wp-content/uploads/sites/3/2021/09/Les-differents-types-de-reseaux-de-neurones-RNN-LSTM-et-GRU-1024x307.jpg)\n",
    "\n",
    "![Title](https://upload.wikimedia.org/wikipedia/commons/6/63/Long_Short-Term_Memory.svg)\n",
    "\n",
    "\n",
    "https://penseeartificielle.fr/comprendre-lstm-gru-fonctionnement-schema/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtTqcfyRbxsA"
   },
   "source": [
    "L'approche est traditionnelle: \n",
    "\n",
    "- Charger une corpus d'entrainement\n",
    "- Filtrer ce texte\n",
    "- Nettoyer intelligemment le texte\n",
    "- Tokenizer les mots puis les textes (qui seront alors un sequence de mots)\n",
    "- Normaliser la longueur des textes donnés en entrée du modele\n",
    "- Créer le modele de LSTM\n",
    "- Entrainer le modele\n",
    "- Generer du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Bz4kLTfbxsB"
   },
   "source": [
    "#### Charger une corpus d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T22:30:52.003824Z",
     "start_time": "2022-05-22T22:30:47.952759Z"
    },
    "executionInfo": {
     "elapsed": 4070,
     "status": "ok",
     "timestamp": 1652692134244,
     "user": {
      "displayName": "Mehdi Rhouz",
      "userId": "12499758823947150359"
     },
     "user_tz": -120
    },
    "id": "EclJcJmlbxsC"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Utilisons keras  pour construire notre Réseau\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.utils as ku \n",
    "\n",
    "from numpy.random import seed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os \n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T22:30:56.591291Z",
     "start_time": "2022-05-22T22:30:56.087230Z"
    },
    "executionInfo": {
     "elapsed": 552,
     "status": "ok",
     "timestamp": 1652692138523,
     "user": {
      "displayName": "Mehdi Rhouz",
      "userId": "12499758823947150359"
     },
     "user_tz": -120
    },
    "id": "tiyQurBWzTLO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_excel('LOR_3.xlsx')\n",
    "text='. '.join(df['Commentaire'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T22:30:56.996609Z",
     "start_time": "2022-05-22T22:30:56.968228Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1652692139548,
     "user": {
      "displayName": "Mehdi Rhouz",
      "userId": "12499758823947150359"
     },
     "user_tz": -120
    },
    "id": "OOUDJsmSEphY",
    "outputId": "90ada6b5-d354-40ac-b7cb-fbb58e8ebb28"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Score</th>\n",
       "      <th>Commentaire</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11 juin 2013</td>\n",
       "      <td>5,0</td>\n",
       "      <td>Commentaire valable pour l'ensemble de la tril...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 mai 2013</td>\n",
       "      <td>5,0</td>\n",
       "      <td>3h fantastiques, avec scènes de bataille épiqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 juin 2013</td>\n",
       "      <td>5,0</td>\n",
       "      <td>Si tu as aimé La Communauté de l'Anneau et Les...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11 novembre 2013</td>\n",
       "      <td>5,0</td>\n",
       "      <td>\"Le retour du roi\" clôture la trilogie du Seig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14 octobre 2013</td>\n",
       "      <td>5,0</td>\n",
       "      <td>L'épisode final de la légendaire sage du Seign...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Data Score                                        Commentaire\n",
       "0      11 juin 2013   5,0  Commentaire valable pour l'ensemble de la tril...\n",
       "1        1 mai 2013   5,0  3h fantastiques, avec scènes de bataille épiqu...\n",
       "2       3 juin 2013   5,0  Si tu as aimé La Communauté de l'Anneau et Les...\n",
       "3  11 novembre 2013   5,0  \"Le retour du roi\" clôture la trilogie du Seig...\n",
       "4   14 octobre 2013   5,0  L'épisode final de la légendaire sage du Seign..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T22:37:28.328747Z",
     "start_time": "2022-05-22T22:37:28.328747Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2171,
     "status": "ok",
     "timestamp": 1652692144740,
     "user": {
      "displayName": "Mehdi Rhouz",
      "userId": "12499758823947150359"
     },
     "user_tz": -120
    },
    "id": "f7bprGNtDiy_",
    "outputId": "a7646b77-6f61-4bd9-f292-6c311c5f5c81"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T22:31:08.668071Z",
     "start_time": "2022-05-22T22:31:08.645724Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "executionInfo": {
     "elapsed": 306,
     "status": "error",
     "timestamp": 1652691201206,
     "user": {
      "displayName": "Mehdi Rhouz",
      "userId": "12499758823947150359"
     },
     "user_tz": -120
    },
    "id": "ovVyR43GbxsD",
    "outputId": "5a3cdb8d-ce29-4102-f9d6-6aa4b8046b7c"
   },
   "outputs": [],
   "source": [
    "# Pour la bible \n",
    "with open('la_bible_nouveau_testament.txt','r',encoding='utf8') as f:\n",
    "    text=f.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EK5kEXmbxsD"
   },
   "source": [
    "#### Nettoyer intelligemment le texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T22:31:10.741854Z",
     "start_time": "2022-05-22T22:31:10.518240Z"
    },
    "executionInfo": {
     "elapsed": 203,
     "status": "ok",
     "timestamp": 1652692440712,
     "user": {
      "displayName": "Mehdi Rhouz",
      "userId": "12499758823947150359"
     },
     "user_tz": -120
    },
    "id": "Ay685k1KbxsE"
   },
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "stop=[]\n",
    "\n",
    "\n",
    "def tok_me(texte):\n",
    "    # J'enleve la ponctuation et je mets en minuscule\n",
    "    #p=\"([aA-zZéèàùîêâûôçïëœ0-9]{1,})\"\n",
    "    p=\"([aA-zZéèàùîêâûôçïëœ.,:!?;]{1,})\"\n",
    "    texte=' '.join(re.findall(p,texte))\n",
    "    #Je renvoi une liste de token\n",
    "    return texte.lower().split()\n",
    "\n",
    "def stop_me(liste_token):\n",
    "    final=[]\n",
    "    for token in liste_token:\n",
    "        if token in stop:\n",
    "            continue\n",
    "        final.append(token)\n",
    "    return final\n",
    "\n",
    "def preprocess(texte):\n",
    "    return ' '.join(stop_me(tok_me(texte)))\n",
    "\n",
    "#Corpus=df['Commentaire']\n",
    "\n",
    "texte=[elem for elem in re.split('!|\\?|\\.{1,}| \\-{1,}|;|:', text.replace('.',' . ').replace(',',' , ').replace(':',' : ').replace('?',' ? ')) if (len(str(elem).split()) < 100)]\n",
    "\n",
    "com=list(map(preprocess,texte))\n",
    "\n",
    "\n",
    "# On prend les phrases avec + 2 mots: \n",
    "com = [elem for elem in com if len(elem.split()) > 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T22:31:11.972214Z",
     "start_time": "2022-05-22T22:31:11.955864Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1652692447029,
     "user": {
      "displayName": "Mehdi Rhouz",
      "userId": "12499758823947150359"
     },
     "user_tz": -120
    },
    "id": "7znng3hHbxsE",
    "outputId": "7627e5d3-53c7-4e5b-e028-10f2179ed1d2",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('et aram engendra aminadab', 19030)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com[7],len(texte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3m1yXamSbxsE"
   },
   "source": [
    "#### Tokenizer les mots puis les textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "js4TlQFfbxsE"
   },
   "source": [
    "Le but de cette modelisation est de prédire le/les mots suivant une sequence (mots/phrases) donnée en entrée.\n",
    "\n",
    "Dans la fonction suivante, nous allons utiliser le Tokenizer de keras pour extraire les elements uniques (mot) de chaque phrase puis leur donner un id.\n",
    "\n",
    "Nous allons, dans inp_sequences, ecrire toutes les combinaisons (n_gram) de token pour chaque phrase.\n",
    "\n",
    "ex : \"Je vais bien merci beaucoup\"\n",
    "\n",
    "[Je,vais]\n",
    "[Je,vais,bien]\n",
    "[Je vais bien merci] \n",
    "etc...\n",
    "\n",
    "C'est comme cela que nous preparons l'entrainement : telle sequence d'id doit me donner le suivant, etc ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T22:31:15.993770Z",
     "start_time": "2022-05-22T22:31:15.806453Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 850,
     "status": "ok",
     "timestamp": 1652693068336,
     "user": {
      "displayName": "Mehdi Rhouz",
      "userId": "12499758823947150359"
     },
     "user_tz": -120
    },
    "id": "8P9yDsuEbxsF",
    "outputId": "fe69f0e7-c71f-484f-9ac8-92a76c8c847a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46755,\n",
       " [[112, 23],\n",
       "  [112, 23, 9],\n",
       "  [112, 23, 9, 8],\n",
       "  [112, 23, 9, 8, 54],\n",
       "  [112, 23, 9, 8, 54, 9],\n",
       "  [112, 23, 9, 8, 54, 9, 8],\n",
       "  [112, 23, 9, 8, 54, 9, 8, 193],\n",
       "  [112, 23, 9, 8, 54, 9, 8, 193, 1593],\n",
       "  [112, 23, 9, 8, 54, 9, 8, 193, 1593, 204],\n",
       "  [112, 23, 9, 8, 54, 9, 8, 193, 1593, 204, 8]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "    #Tokenizer : associe à chaque mot unique un ID \n",
    "    \n",
    "    ## Text to sequence of tokens (mots uniques) \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences, total_words\n",
    "\n",
    "inp_sequences, total_words = get_sequence_of_tokens(random.sample(com, 3600))\n",
    "len(inp_sequences),inp_sequences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMnrC-dtbxsF"
   },
   "source": [
    "#### Nous pouvons traduire ces sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T22:31:20.105234Z",
     "start_time": "2022-05-22T22:31:20.096422Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 204,
     "status": "ok",
     "timestamp": 1652693070388,
     "user": {
      "displayName": "Mehdi Rhouz",
      "userId": "12499758823947150359"
     },
     "user_tz": -120
    },
    "id": "CcapGSPSbxsF",
    "outputId": "8f3f4ae4-4518-4c77-b935-fd182e7790b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['non', 'mais'], ['non', 'mais', 'que'], ['non', 'mais', 'que', 'les'], ['non', 'mais', 'que', 'les', 'choses'], ['non', 'mais', 'que', 'les', 'choses', 'que'], ['non', 'mais', 'que', 'les', 'choses', 'que', 'les'], ['non', 'mais', 'que', 'les', 'choses', 'que', 'les', 'nations'], ['non', 'mais', 'que', 'les', 'choses', 'que', 'les', 'nations', 'sacrifient'], ['non', 'mais', 'que', 'les', 'choses', 'que', 'les', 'nations', 'sacrifient', 'elles'], ['non', 'mais', 'que', 'les', 'choses', 'que', 'les', 'nations', 'sacrifient', 'elles', 'les']]\n"
     ]
    }
   ],
   "source": [
    "print(list(map((lambda x: [tokenizer.index_word[elem] for elem in x]), inp_sequences[:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BdbQhkcbxsF"
   },
   "source": [
    "#### Normaliser la longueur des textes donnés en entrée du modele"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pR27FjJbxsF"
   },
   "source": [
    "Dans la cellule suivante, nous creons l'echantillon de train sur la base des in_sequence generées plus haut.\n",
    "\n",
    "La subtilité, c'est de pouvoir livrer à la machine une donnée d'entrée toujours de meme taille/format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T22:31:24.542544Z",
     "start_time": "2022-05-22T22:31:24.334692Z"
    },
    "executionInfo": {
     "elapsed": 2316,
     "status": "ok",
     "timestamp": 1652693119947,
     "user": {
      "displayName": "Mehdi Rhouz",
      "userId": "12499758823947150359"
     },
     "user_tz": -120
    },
    "id": "9ow4q8K0bxsG"
   },
   "outputs": [],
   "source": [
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T22:31:24.776839Z",
     "start_time": "2022-05-22T22:31:24.762585Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1652691508634,
     "user": {
      "displayName": "Mehdi Rhouz",
      "userId": "12499758823947150359"
     },
     "user_tz": -120
    },
    "id": "lbOwV4kCbxsG",
    "outputId": "71c47a17-c7d6-4fb9-8280-b40248a961e9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'input [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0  112   23    9    8   54\n",
      "    9    8  193 1593  204    8 1593    6   19  483    1] tout\n",
      "L'output, c'est à dire : 112  soit : non\n"
     ]
    }
   ],
   "source": [
    "i=15\n",
    "print(\"L'input\",predictors[i],tokenizer.index_word[predictors[i].argmax()])\n",
    "print(\"L'output, c'est à dire :\",label[i].argmax(),\" soit :\", tokenizer.index_word[label[i].argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ik8MV99dbxsG"
   },
   "source": [
    "La sortie est un vecteur one hot encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hw6MrfEtbxsG"
   },
   "source": [
    "#### Créer le modele de LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-NURVCzbxsG"
   },
   "source": [
    "Nous avons ainsi nos trains (predictor) et le target associé (label)\n",
    "\n",
    "Input Layer : Sequence de mots\n",
    "\n",
    "Petite couche d'embedding \n",
    "\n",
    "LSTM Layer : Determine la sortie via les cellules de LSTM \n",
    "\n",
    "Dropout Layer :Une reductions du nombre de cellules (regularisation) qui va aleatoirement eteindre les neurones LSTM instables ==> Eviter l'overfitting \n",
    "\n",
    "Output Layer : Calcule une probabilité (softmax !) du meilleur choix en sortie \n",
    "\n",
    "\n",
    "![title](http://www.shivambansal.com/blog/text-lstm/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGwIsQXPbxsG"
   },
   "source": [
    "#### Subtilité Bidirectionnal LSTM\n",
    "\n",
    "Le modèle prédit un mot en fonction à la fois des termes qui le précèdent et  de ceux qui lui succèdent. La fonction de coût est une simple moyenne des fonctions de coût calculées par les deux parties du bi-LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T22:31:28.849921Z",
     "start_time": "2022-05-22T22:31:28.841365Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1652694119343,
     "user": {
      "displayName": "Mehdi Rhouz",
      "userId": "12499758823947150359"
     },
     "user_tz": -120
    },
    "id": "bi9GPJ5MbxsG",
    "outputId": "2ab8829c-ba40-4959-bada-f9ba1152fc6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5277, 82)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words,max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T22:31:30.888161Z",
     "start_time": "2022-05-22T22:31:30.179730Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 958,
     "status": "ok",
     "timestamp": 1652695508734,
     "user": {
      "displayName": "Mehdi Rhouz",
      "userId": "12499758823947150359"
     },
     "user_tz": -120
    },
    "id": "jjc3ZYfybxsH",
    "outputId": "cde585b5-3881-45c6-bb02-446e7dab962b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 81, 20)\n",
      "(None, 5277)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 81, 20)            105540    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 96)               26496     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5277)              511869    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 643,905\n",
      "Trainable params: 643,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.optimizers import SGD,Adamax,Adagrad,RMSprop\n",
    "\n",
    "\n",
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input Embedding Layer : Transforme les entiers positifs (index) en vecteurs de float de taille fixe.\n",
    "    # La sequence d'input ([0 0 0 id3 id2 id1]) ==> ([Emb(0 Emb(0) ... Emb(id3) Emb(id2 Emb(id1))])\n",
    "    model.add(Embedding(total_words, 20, input_length=input_len))\n",
    "    print(model.output_shape)\n",
    "    \n",
    "    # LSTM Layer nous retournons la sequence entiere à la couche LSTM suivante qui en a besoin\n",
    "    #model.add(Bidirectional(LSTM(48, dropout=0.1, return_sequences=True), input_shape=(input_len, 1)))\n",
    "    #model.add(LSTM(64,return_sequences=True,dropout=0.1))\n",
    "    # LSTM Layer \n",
    "    #model.add(LSTM(64,return_sequences=False,dropout=0.15))\n",
    "    model.add(Bidirectional(LSTM(48, return_sequences=False,dropout=0.1), input_shape=(input_len, 1)))\n",
    "    \n",
    "    #model.add(Dense(model.output_shape[-1], activation='tanh'))\n",
    "    # Output Layer recherche du meilleur candidat sur un vecteur de la taille du vocabulaire\n",
    "    # Vecteur de sortie ([score1,score2,score3,...., score_nb_word_vocab].argmax ==> id ayant le score max)\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    \n",
    "    # wi(i+1) = wi (i) - alpha x (d E / d wi)\n",
    "    opt=Adamax(learning_rate=0.09)\n",
    "    model.compile(loss='categorical_crossentropy',metrics=['accuracy'], optimizer=opt)\n",
    "    print(model.output_shape)\n",
    "    return model\n",
    "\n",
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0S47DQk9bxsH"
   },
   "source": [
    "#### Entrainer le modele\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T22:43:03.710087Z",
     "start_time": "2022-05-22T22:37:38.278950Z"
    },
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "c14OastFbxsH",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "749/749 [==============================] - 73s 97ms/step - loss: 4.6834 - accuracy: 0.1683 - val_loss: 5.6536 - val_accuracy: 0.1555\n",
      "Epoch 2/20\n",
      "749/749 [==============================] - 74s 99ms/step - loss: 4.4279 - accuracy: 0.1804 - val_loss: 5.7104 - val_accuracy: 0.1562\n",
      "Epoch 3/20\n",
      "749/749 [==============================] - 74s 99ms/step - loss: 4.2054 - accuracy: 0.1922 - val_loss: 5.8330 - val_accuracy: 0.1546\n",
      "Epoch 4/20\n",
      "749/749 [==============================] - 74s 99ms/step - loss: 4.0427 - accuracy: 0.2025 - val_loss: 5.9941 - val_accuracy: 0.1575\n",
      "Epoch 5/20\n",
      "749/749 [==============================] - 74s 99ms/step - loss: 3.8958 - accuracy: 0.2115 - val_loss: 6.1294 - val_accuracy: 0.1448\n"
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\n",
    "\n",
    "\n",
    "history = model.fit(predictors, label,\n",
    "    epochs=20, \n",
    "    batch_size=50,validation_split=0.2,callbacks=[callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8oVBZRpbxsH"
   },
   "source": [
    "#### Générer du texte\n",
    "\n",
    "Enfin, pour la prediction, une fonction qui tokenize le debut du texte, qui retrouve les ids mots associés, qui predit le mot le plus probable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T22:49:43.832074Z",
     "start_time": "2022-05-22T22:49:43.818917Z"
    },
    "id": "vIOj6q_ObxsH"
   },
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for i in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        #predicted = model.predict_classes(token_list, verbose=0)\n",
    "        predict_x=model.predict(token_list) \n",
    "        predicted=np.argmax(predict_x,axis=1)\n",
    "        #print(\"l'index du score max du on hot encoding de sortie du modele\", predicted)\n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uX0_0E-bxsH"
   },
   "source": [
    "un exemple d'appel à cette fonction prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T22:49:49.178713Z",
     "start_time": "2022-05-22T22:49:47.271001Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jxjRg3xqbxsI",
    "outputId": "a8996c93-2ee5-4087-8f7d-acd3c886e1ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "tout est incroyable, du début à la fin. j ai adoré fait connaître à la droite de dieu et\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "ce film est vraiment le meilleur de la trilogie gloire et que nous avons trouvé l assemblée\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "le seigneur des anneaux est une longue histoire, ce dernier épisode est toujours et de la gloire et que nous avons\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "j ai vraiment détesté, ce dernier en moi amène un corps ne sont pas\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('##########################################')\n",
    "print (generate_text(preprocess(\"Tout est incroyable, du début à la fin. J'ai adoré\"), 8, model, max_sequence_len))\n",
    "print (generate_text(preprocess(\"Ce film est vraiment le meilleur de la trilogie\"), 8, model, max_sequence_len))\n",
    "print (generate_text(preprocess(\"Le seigneur des anneaux est une longue histoire, ce dernier épisode est toujours\"), 8, model, max_sequence_len))\n",
    "print (generate_text(preprocess(\"J'ai vraiment détesté, ce dernier\"), 8, model, max_sequence_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ou6V7Zk8bxsI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Génération_de_texte_et_LSTM_variante_BiLSTM_ennonce.ipynb",
   "version": ""
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
